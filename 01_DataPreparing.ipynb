{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DataPreparing\r\n",
        "\r\n",
        "The first step of our notebooks will be to run a DataPreparing script.  \r\n",
        "This contains all the necessary code to transform our original images into images that are ready for AI training.  \r\n",
        "\r\n",
        "To benefit from the perks of our Azure cloud service, we will be creating a new dataset to store our processed images.\r\n",
        "\r\n",
        "If you want more background information on these notebooks, the HackMD document will help you as well."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\r\n",
        "\r\n",
        "Our virtual machine might not have all packages installed yet. So let's go and install some packages.  \r\n",
        "We can use cell-magic for this, which will allow us to stay inside this notebook and just executing the cells.  \r\n",
        "\r\n",
        "Later on, these cells might nog be necessary anymore, which is why we include it at the top. During other builds, you can just ignore these.\r\n",
        "\r\n",
        "As a best practice, let's make sure to only work on the version we know is safe. This is a great way to organising our AI projects. By keeping the versions linked like this, no unexpected new version would break our code!"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\r\n",
        "\r\n",
        "opencv-python==4.5.4.60"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We want to make sure to use the right AzureML packages!\r\n",
        "!/anaconda/envs/azureml_py38_PT_TF/bin/python -m pip install -r requirements.txt"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655912458118
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell can be used to fill in some values that you will be referring to in the coming cells\r\n",
        "train_test_split_factor = 0.20"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1655912458269
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the default packages for data processing and visualisation\n",
        "import numpy as np # Used to process our images in a data-format\n",
        "import matplotlib.pyplot as plt # visualise the images\n",
        "import cv2 # Process the images\n",
        "\n",
        "\n",
        "import os\n",
        "from glob import glob\n",
        "import math\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") # Warnings that can be ignored will be ignored\n",
        "\n",
        "import random\n",
        "SEED = 42 # Everytime you want to randomize items, use this `random.seed(SEED)` option. This way, you are always having the same randomization as I have.\n",
        "random.seed(SEED)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1655912458904
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import AzureML packages\r\n",
        "from azureml.core import Workspace\r\n",
        "from azureml.core import Dataset\r\n",
        "from azureml.data.datapath import DataPath\r\n",
        "from azureml.core.compute import AmlCompute\r\n",
        "from azureml.core.compute import ComputeTarget"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655912459083
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Connecting to the Azure ML Workspace\r\n",
        "\r\n",
        "Azure Machine Learning needs to connect through the Azure SDK with the Workspace object. This contains all the information inside of this 'Laboratory'"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The information below should reflect your situation regarding Azure. You should have a ResourceGroup called 'TETRA-Workshop-2406' and a workspace name called 'segersnathan' if you followed my instructions on HackMD.\r\n",
        "The subscription ID, however, is something that has been created by Azure itself.\r\n",
        "\r\n",
        "Luckily, this ML studio gives us a quick way to find this information.\r\n",
        "Click on the \\/-arrow in the upper-right corner over there â†—ï¸, next to your profile picture.\r\n",
        "\r\n",
        "Most of your information is in there as well, but you still can't find your subscription_**id** there ...\r\n",
        "\r\n",
        "Press the 'Download config' option, and you'll be left with this information:\r\n",
        "\r\n",
        "```json\r\n",
        "{\r\n",
        "    \"subscription_id\": \"763622cd-d9e1-46f1-84c7-635df9708641\",\r\n",
        "    \"resource_group\": \"TETRA-Workshop-2406\",\r\n",
        "    \"workspace_name\": \"nathan-segers-ml\"\r\n",
        "}\r\n",
        "```\r\n",
        "\r\n",
        "Which gives you exactly the information you need ðŸ¥°\r\n",
        "\r\n",
        "There's also an option to use this configuration itself. Search for the documentation on how to do it: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace(class)?view=azure-ml-py\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Either get environment variables, or a fallback name, which is the second parameter.\n",
        "## Currently, fill in the fallback values. Later on, we will make sure to work with Environment values. So we're already preparing for it in here!\n",
        "workspace_name = os.environ.get('WORKSPACE', 'MLOps-Workshop')\n",
        "subscription_id = os.environ.get('SUBSCRIPTION_ID', '763622cd-d9e1-46f1-84c7-635df9708641')\n",
        "resource_group = os.environ.get('RESOURCE_GROUP', 'TETRA-Workshop-2406')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1655912459277
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ws = Workspace.get(name=workspace_name,\n",
        "               subscription_id=subscription_id,\n",
        "               resource_group=resource_group)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1655912459429
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 -- Data preparing\r\n",
        "\r\n",
        "Let's get started preparing our data.\r\n",
        "The original dataset contains images of Cats, Dogs and Pandas, but are not yet ready for AI processing.\r\n",
        "They are all different shapes and sizes, so they need to be processed.\r\n",
        "\r\n",
        "I could've done it earlier on and given you the processed data, but that's not really that fun, isn't it ðŸ˜‰\r\n",
        "\r\n",
        "By performing these steps, you see how you can create datasets in a Programmatoric way, and upload new data.\r\n",
        "We will make use of the Azure Machine Learning tools to easily manage all of our datasets in a structured way.\r\n",
        "\r\n",
        "## Step 2.1 -- Checking our data\r\n",
        "\r\n",
        "Let us first explore how the data looks. We'll create 3 subdirectories under a data directory, one for each animal.\r\n",
        "If you want to update this to more animals later, simply adapt the `ANIMALS` list."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ANIMALS = ['cats', 'dogs', 'pandas'] # As we have three classes, let's just defined them hoor. This way we can easily loop over them later.\r\n",
        "# We can also work with ENUM's if you want another fun way of accessing properties:\r\n",
        "# Feel free to uncomment these lines, I won't work with them, but you can always do so if you prefer\r\n",
        "\r\n",
        "\r\n",
        "# from enum import Enum\r\n",
        "# Animals = Enum('Animal', 'CATS DOGS PANDAS')\r\n",
        "\r\n",
        "# print(list(Animals))\r\n",
        "# print(Animals.CATS.name.lower()) # You can always get their effective string values this way"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1655912459564
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will need to create temporary directories to store the images while we process them.\r\n",
        "This script will create a `data` folder, and then make subdirectories for each animal."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_folder = os.path.join(os.getcwd(), 'data')\n",
        "os.makedirs(data_folder, exist_ok=True)\n",
        "for animal_name in ANIMALS:\n",
        "    os.makedirs(os.path.join(data_folder, 'animals', animal_name), exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1655912459827
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all the datasets that were registered in the UI\n",
        "# We can then easily select the ones we need\n",
        "datasets = Dataset.get_all(workspace=ws) # Make sure to give our workspace with it\n",
        "print(datasets)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1655912459995
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare to show some test images!"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "def downloadTestImagesForAnimal(animal_name: str, amount_of_test_images: int) -> List[str]:\n",
        "    \"\"\"\n",
        "    Download test images from an Azure dataset.\n",
        "    Choose the amount of test images per animal you want.\n",
        "    The images will be downloaded to the `data/<animal>` directory\n",
        "\n",
        "    Parameters:\n",
        "        animal_name (str): The name of the dataset / animal to download images from\n",
        "        amount_of_test_images (int): The amount of test images to download\n",
        "    Returns:\n",
        "        file_paths (List[str]): list of file paths to display the images\n",
        "    \"\"\"\n",
        "    animal_dataset = datasets[animal_name]\n",
        "    test_images = animal_dataset.take(amount_of_test_images)\n",
        "    file_paths = test_images.download(os.path.join(data_folder, 'animals', animal_name), overwrite=True)\n",
        "    return file_paths\n",
        "\n",
        "def displayTestImages(images: List[str]) -> None:\n",
        "    fig, axes = plt.subplots(1, len(images), figsize=(12, 9), sharey=True)\n",
        "    axes_1d = axes.ravel()\n",
        "    for i in range(0, len(images)):    \n",
        "        axes_1d[i].imshow(plt.imread(images[i]), cmap='gray')\n",
        "        axes_1d[i].axis('off')\n",
        "        \n",
        "    plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1655912460158
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose a number of images you want to show the users"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "animal_images = {}\n",
        "AMOUNT_OF_IMAGES = 5\n",
        "for animal_name in ANIMALS:\n",
        "    print(f\"Downloading {AMOUNT_OF_IMAGES} images of {animal_name}...\")\n",
        "    animal_images[animal_name] = {}\n",
        "    animal_images[animal_name]['images'] = downloadTestImagesForAnimal(animal_name, AMOUNT_OF_IMAGES)\n",
        "    "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1655912465735
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "displayTestImages(animal_images['pandas']['images'])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1655912466553
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2.2 Processing and uploading the resized images\r\n",
        "\r\n",
        "Like we said in the introduction, we'll need to resize the images so they're all the same shape and size.\r\n",
        "We are getting ready for a CNN model, which will require an input of 64, 64. You can go higher, but then it'll take longer to train, but also improve the accuracy."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a mounth point. Think of it like your D:/ drive on your PC\r\n",
        "mount_path = os.path.join(os.getcwd(), 'mount')\r\n",
        "os.makedirs(mount_path, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655912466755
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defien a path to store the animal images onto. We'll choose for `data/processed/animals` this time. Again, create subdirectories for all the animals\r\n",
        "processed_path = os.path.join(os.getcwd(), 'data', 'processed', 'animals')\r\n",
        "os.makedirs(processed_path, exist_ok=True)\r\n",
        "for animal_name in ANIMALS:\r\n",
        "    os.makedirs(os.path.join(processed_path, animal_name), exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655912466906
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mounts = []"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655912467057
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655912467298
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mountProcessAndUploadAnimalImages(animal_name):\r\n",
        "\r\n",
        "    # The mount context is to load in the dataset to our directory.\r\n",
        "    # Make sure to stop it when it's finished!\r\n",
        "\r\n",
        "    # Get the dataset name for this animal, then mount to the directory\r\n",
        "    mounted_context = datasets[animal_name].mount(mount_path)\r\n",
        "    print('Starting the Mount context, to get all the original images.')\r\n",
        "    mounted_context.start()\r\n",
        "    mounts.append(mounted_context)\r\n",
        "\r\n",
        "    # Get all the image paths with the `glob()` method.\r\n",
        "    print(f'Resizing all images for {animal_name} ...')\r\n",
        "    imagePaths = glob(f\"{mount_path}/*.jpg\")\r\n",
        "\r\n",
        "    # Process all the images with OpenCV. Reading them, then resizing them to 64x64 and saving them once more.\r\n",
        "    print(f\"Processing {len(imagePaths)} images\")\r\n",
        "    for imagePath in imagePaths:\r\n",
        "        image = cv2.imread(imagePath)\r\n",
        "        image = cv2.resize(image, (64, 64)) # Resize to a square of 64, 64\r\n",
        "        cv2.imwrite(os.path.join(processed_path, animal_name, imagePath.split('/')[-1]), image)\r\n",
        "    print(f'... done resizing. Stopping context now...')\r\n",
        "    \r\n",
        "    # Upload the directory as a new dataset\r\n",
        "    print(f'Uploading directory now ...')\r\n",
        "    resized_dataset = Dataset.File.upload_directory(\r\n",
        "                        # Enter the source directory on our machine where the resized pictures are\r\n",
        "                        src_dir = os.path.join(processed_path, animal_name),\r\n",
        "                        # Create a DataPath reference where to store our images to. We'll use the default datastore for our workspace.\r\n",
        "                        target = DataPath(datastore=ws.get_default_datastore(), path_on_datastore=f'processed_animals/{animal_name}'),\r\n",
        "                        overwrite=True)\r\n",
        "    # Make sure to register the dataset whenever everything is uploaded.\r\n",
        "    resized_dataset.register(ws,\r\n",
        "                            name=f'resized_{animal_name}',\r\n",
        "                            description=f'{animal_name} images resized tot 64, 64',\r\n",
        "                            tags={'animals': animal_name, 'AI-Model': 'CNN'}, # Optional tags, can always be interesting to keep track of these!\r\n",
        "                            create_new_version=True)\r\n",
        "    print(f'... Done')\r\n",
        "    # Stop the context now.\r\n",
        "    mounted_context.stop()\r\n",
        "    print(f\"... Context stopped and freed.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655912467448
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo umount \"/mnt/batch/tasks/shared/LS_root/mounts/clusters/nathan1/code/Users/nathan_segers/04-AzureML-Source/mount\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\r\n",
        "# Process all the animal images now. This can take a while!\r\n",
        "# We'll use Cell magic once more, to time how long this takes!\r\n",
        "mountProcessAndUploadAnimalImages('cats')\r\n",
        "mountProcessAndUploadAnimalImages('dogs')\r\n",
        "# mountProcessAndUploadAnimalImages('pandas')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1640004384857
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our Animal datasets are now registered onto the datasets of Azure, which is what we need. We can now safely delete all the images from our disk."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2.3 -- Create Training and Testing splitted data\r\n",
        "\r\n",
        "As you might remember, for AI projects we want to split up our AI data in a batch of Training and Testing data, with the necessary labels.\r\n",
        "We will take a portion of our data to train on, and the rest will be kept to test and validate our AI model.\r\n",
        "\r\n",
        "For this, I chose a percentage in the first cell:\r\n",
        "`train_test_split_factor = 0.20`\r\n",
        "\r\n",
        "Thus, we will take 80% of our data to train on, and 20% to test our model and evaluate the results."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_datapaths = []\r\n",
        "testing_datapaths = []\r\n",
        "default_datastore = ws.get_default_datastore()\r\n",
        "for animal_name in ANIMALS:\r\n",
        "    # Get the dataset by name\r\n",
        "    animal_dataset = Dataset.get_by_name(ws, f\"resized_{animal_name}\")\r\n",
        "\r\n",
        "    # Get only the .JPG images\r\n",
        "    animal_images = [img for img in animal_dataset.to_path() if img.split('.')[-1] == 'jpg']\r\n",
        "\r\n",
        "    ## Concatenate the names for the animal_name and the img_path. Don't put a / between, because the img_path already contains that\r\n",
        "    animal_images = [(default_datastore, f'processed_animals/{animal_name}{img_path}') for img_path in animal_images] # Make sure the paths are actual DataPaths\r\n",
        "    \r\n",
        "    random.seed(SEED) # Use the same random seed as I use and defined in the earlier cells\r\n",
        "    random.shuffle(animal_images) # Shuffle the data so it's randomized\r\n",
        "    \r\n",
        "    ## Testing images\r\n",
        "    amount_of_test_images = math.ceil(len(animal_images) * train_test_split_factor) # Get a small percentage of testing images\r\n",
        "\r\n",
        "    animal_test_images = animal_images[:amount_of_test_images]\r\n",
        "    animal_training_images = animal_images[amount_of_test_images:]\r\n",
        "    \r\n",
        "    # Add them all to the other ones\r\n",
        "    testing_datapaths.extend(animal_test_images)\r\n",
        "    training_datapaths.extend(animal_training_images)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655912927853
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2.4 -- Register Training and Testing Dataset\r\n",
        "\r\n",
        "We don't need to re-upload our images, but we do need to register our Training and Testing datasets as well."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = Dataset.File.from_files(path=training_datapaths)\r\n",
        "testing_dataset = Dataset.File.from_files(path=testing_datapaths)\r\n",
        "\r\n",
        "training_dataset = training_dataset.register(ws,\r\n",
        "    name=f'animals-training-set',\r\n",
        "    description=f'The Animal Images to train, resized tot 64, 64',\r\n",
        "    tags={'animals': ','.join(ANIMALS), 'AI-Model': 'CNN', 'Split size': str(1 - train_test_split_factor), 'type': 'training'},\r\n",
        "    create_new_version=True)\r\n",
        "\r\n",
        "testing_dataset = testing_dataset.register(ws,\r\n",
        "    name=f'animals-testing-set',\r\n",
        "    description=f'The Animal Images to test, resized tot 64, 64',\r\n",
        "    tags={'animals': ','.join(ANIMALS), 'AI-Model': 'CNN', 'Split size': str(train_test_split_factor), 'type': 'testing'},\r\n",
        "    create_new_version=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655912952920
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finalizing\r\n",
        "\r\n",
        "Now that we have all our datasets uploaded, we can continue to the next notebook where we will learn how to Train an AI model into Azure Machine Learning."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One last check for the datasets\r\n",
        "Dataset.get_all(ws)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1655912986193
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "5826022c13cf670f25f82c54601002af42321e6e77593e3891ed52ffe2de205c"
    },
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "orig_nbformat": 4,
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}